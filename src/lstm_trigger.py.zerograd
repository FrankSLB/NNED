import torch
import torch.autograd as autograd
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np

torch.manual_seed(1)

class LSTMTrigger(nn.Module):
    def __init__(self, model_params, args):
        super(LSTMTrigger, self).__init__()
        vocab_size, tagset_size, embedding_dim, random_dim, pretrain_embedding = model_params
        gpu = args.gpu

        self.random_embed = False
        self.lstm_hidden_dim = args.hidden_dim
        self.bilstm_flag = args.bilstm
        self.lstm_layer = args.num_layers
        self.batch_size = args.batch_size
        self.batch_mode = True if args.batch_size>1 else False

        self.use_position = args.use_position
        self.position_size = 100
        self.position_dim = 5

        self.hidden_dim_fst = self.lstm_hidden_dim
        if self.bilstm_flag: self.hidden_dim_fst *= 2

        if args.hidden_dim_snd == 0: self.hidden_dim_snd = self.hidden_dim_fst
        else: self.hidden_dim_snd = args.hidden_dim_snd

        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.word_embeddings.weight.data.copy_(torch.from_numpy(pretrain_embedding))
        self.position_embeddings = nn.Embedding(self.position_size, self.position_dim)

        self.drop = nn.Dropout(args.dropout)
        self.lstm = nn.LSTM(embedding_dim, self.lstm_hidden_dim, num_layers=self.lstm_layer, bidirectional=self.bilstm_flag)
        self.fst_hidden = nn.Linear(self.hidden_dim_fst, self.hidden_dim_snd)
        self.hidden2tag = nn.Linear(self.hidden_dim_snd, tagset_size)
        self.hidden2tag_iden = nn.Linear(self.hidden_dim_snd, 2)

        if gpu:
            self.drop = self.drop.cuda()
            self.word_embeddings = self.word_embeddings.cuda()
            self.position_embeddings = self.position_embeddings.cuda()
            self.lstm = self.lstm.cuda()
            self.fst_hidden = self.fst_hidden.cuda()
            self.hidden2tag = self.hidden2tag.cuda()
            self.hidden2tag_iden = self.hidden2tag_iden.cuda()

        self.hidden = self.init_hidden(gpu)

    # init hidden of lstm
    def init_hidden(self, gpu):
        dims = (self.lstm_layer, self.batch_size, self.lstm_hidden_dim)
        if self.bilstm_flag:
            dims = (2*self.lstm_layer, self.batch_size, self.lstm_hidden_dim)
        init_value = torch.Tensor(np.random.uniform(-0.01, 0.01, dims))
        #init_value = torch.zeros(dims)
        h0 = autograd.Variable(init_value)
        c0 = autograd.Variable(init_value)
        if gpu:
            h0 = h0.cuda()
            c0 = c0.cuda()
        return (h0,c0)

    # sentence shape: (sent_length, )
    def forward(self, sentence, gpu, debug=False):
        sent_length = sentence.size()[0]
        embeds = self.word_embeddings(sentence)
        if debug:
            print "## word embedding:", type(self.word_embeddings.weight.data), self.word_embeddings.weight.data.size()
            print self.word_embeddings.weight.data[:5, :5]

        # output grad
        grad_debug = True
        if debug and grad_debug:
            if self.word_embeddings.weight.grad is not None:
                print "## word embedding grad:", self.word_embeddings.weight.grad#[:5, :5]
            if embeds.grad is not None:
                print "## sent word embedding grad:", embeds.grad[:5, :5]

        embeds = self.drop(embeds)

        lstm_out, self.hidden = self.lstm(
                embeds.view(sent_length, 1, -1), self.hidden)
        # lstm_out: sent_length * batch_size * hidden_dim
        lstm_out = lstm_out.view(sent_length, -1)
        if debug:
            print "## lstm out:", lstm_out.size(), type(lstm_out.data)
            #print lstm_out.data[:10, :10]
        hidden_in = lstm_out

        if debug:
            print "## hidden in:", hidden_in.data.size(), type(hidden_in.data), hidden_in.view(-1, hidden_in.size(-1)).size()

        hidden_snd = self.fst_hidden(hidden_in.view(-1, hidden_in.size(-1)))
        hidden_snd = F.relu(hidden_snd)
        tag_space = self.hidden2tag(hidden_snd)
        tag_scores = F.log_softmax(tag_space)
        #tag_scores = F.softmax(tag_space)
        tag_space_iden = self.hidden2tag_iden(hidden_snd)
        tag_scores_iden = F.softmax(tag_space_iden)
        return tag_space, tag_scores, tag_space_iden, tag_scores_iden
